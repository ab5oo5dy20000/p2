import cv2
import mediapipe as mp
import numpy as np
import os

# Ø¥Ø¹Ø¯Ø§Ø¯ Mediapipe
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)
mp_draw = mp.solutions.drawing_utils

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´ÙƒÙ„ Ø§Ù„ÙŠØ¯ ÙƒÙ†Ù‚Ø§Ø· Ù…ÙØ·Ø¨Ù‘Ø¹Ø© (Normalized hand shape)
def extract_hand_landmarks_from_result(hand_landmarks):
    points = np.array([[lm.x, lm.y] for lm in hand_landmarks.landmark])
    center = np.mean(points, axis=0)
    norm_points = points - center
    norm_points /= np.linalg.norm(norm_points) + 1e-6
    return norm_points.flatten()

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù…Ù† ØµÙˆØ± Ø§Ù„ÙŠØ¯
def load_reference_data(folder_path):
    reference = []
    for filename in sorted(os.listdir(folder_path)):
        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            img_path = os.path.join(folder_path, filename)
            image = cv2.imread(img_path)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            result = hands.process(image_rgb)
            if result.multi_hand_landmarks:
                for hand_landmarks in result.multi_hand_landmarks:
                    norm_landmarks = extract_hand_landmarks_from_result(hand_landmarks)
                    reference.append(norm_landmarks)
    return reference

# Ø¯Ø§Ù„Ø© Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù…Ø¹ ÙƒÙ„ Ø¥Ø´Ø§Ø±Ø© Ù…Ø±Ø¬Ø¹ÙŠØ©
def is_hello_gesture(current_norm, reference_data, threshold=0.65):
    for ref_norm in reference_data:
        if len(current_norm) != len(ref_norm):
            continue
        diff = np.linalg.norm(current_norm - ref_norm)
        similarity = 1 - diff
        if similarity >= threshold:
            return True
    return False

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©
reference_dir = r"D:\python_prog\dataset\hello"
reference_landmarks = load_reference_data(reference_dir)
print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(reference_landmarks)} ÙˆØ¶Ø¹ÙŠØ© ÙŠØ¯ Ù…Ù† Ù…Ø¬Ù„Ø¯ hello")

# Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
cap = cv2.VideoCapture(0)
print("ğŸ“· Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ØªØ¹Ù…Ù„ Ø§Ù„Ø¢Ù†... Ø§Ø¶ØºØ· q Ù„Ù„Ø®Ø±ÙˆØ¬")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame = cv2.flip(frame, 1)
    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = hands.process(image_rgb)
    text = "I DON'T UNDERSTAND"

    if result.multi_hand_landmarks:
        for hand_landmarks in result.multi_hand_landmarks:
            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            norm_landmarks = extract_hand_landmarks_from_result(hand_landmarks)
            if is_hello_gesture(norm_landmarks, reference_landmarks, threshold=0.65):
                text = "HELLO"

    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©
    cv2.putText(frame, text, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.8, (0, 255, 255), 3)
    cv2.imshow("ğŸ“· Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø¥Ø´Ø§Ø±Ø© HELLO", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
